---
format: 
  pdf:
    documentclass: template/style/uneceart
    include-in-header: 
      - file: preamble.tex
pdf-engine: pdflatex
bibliography: template/UNECE_template_2023SDC.bib
link-citations: true
---

\input{template/UNECE2023cover.tex}

<!-- %% Real content of the paper -->

<!-- % Introduction -->

# Introduction

In recent years, the academic interest in synthetic data has exploded. 
Synthetic data is increasingly being used as a solution to overcome privacy and confidentiality issues that are inherently linked to the dissemination of research data. 
National statistical institutes and other government agencies have started to disseminate synthetic data to the public while restricting access to the original data to protect sensitive information [e.g., @SIPP_Beta_2006; @hawala_synthetic_2008; @drechsler2012]. 
At the same time, researchers start to share a synthetic version of their research data to comply with open science standards [e.g., @vandewiel2023; @obermeyer2019; @zettler2021]. 
Rather than sharing the original research data, a synthetic surrogate is shared to facilitate reviewing the data processing and analysis pipeline. 
Additionally, synthetic data is increasingly being used for training machine learning models [@nikolenko2021]. 
On a lower level, synthetic data can be used in model testing pipelines (before access to the real data is provided), for data exploration (misschien YOUTH citeren all), and for educational purposes.

In short, the general idea of synthetic data is to substitute values from the collected data with synthetic values that are generated from a model. In this way, it is possible to generate an entirely new synthetic data set [commonly referred to as the *fully* synthetic data approach\; @rubin_statistical_1993], but one could also replace only those values that would yield a high risk of disclosure when released [called *partially* synthetic data\; @little_statistical_1993]. Both approaches essentially attempt to build a model that incorporates as much of the information in the real data as possible. The models used to generate synthetic data were originally closely related to methods used for multiple imputation of missing data, such as fully conditional specification [@volker2021] or sequential regression [@nowok2016]. Recently, significant improvements in generative modelling in combination with work on formal privacy guarantees sparked the development of a great deal of novel methods in the computer science community [e.g., @patki2016; @xu_ctgan_2019].



# References
